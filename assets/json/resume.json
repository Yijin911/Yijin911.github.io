{"work":[{"name":"Orby AI - A Uniphore Company","position":"Research Intern","url":"https://www.uniphore.com/orby-ai/","startDate":"2025-05-05","endDate":"2025-08-15","summary":"Studying preference learning for LLM fine-tuning, honored to be advised by Peng Qi.","highlights":["Relativity"]}],"education":[{"institution":"University of Science and Technology of China","location":"Hefei, Anhui, China","area":"Statistics","studyType":"Bachelor","startDate":"2016-08","endDate":"2020-06"},{"institution":"Georgia Institute of Technology","location":"Atlanta, GA","area":"Statistics","studyType":"Ph.D.","startDate":"2020-01"}],"awards":[{"title":"Runner Up","date":"2025-08-26","awarder":"2025 Alice and John Jarvis Ph.D. Student Paper Competition"},{"title":"Mentoring service award for SURE program","date":"2022-08","awarder":"Georgia Institute of Technology"},{"title":"Stewart Fellowship","date":"2021-08","awarder":"Georgia Institute of Technology"},{"title":"Annual outstanding Student Scholarship","startDate":"2017","endDate":"2020","awarder":"University of Science and Technology of China"}],"publications":[{"name":"Abductive Preference Learning","summary":"It proposes a solution for the over-confidence phenomenon for SOTA LLM models, serving as a fine-tuning method that detects the modification of keywords in text prompts or items in image prompts."},{"name":"Kernel-based Equalized Odds: A Quantification of Accuracy Fairness Trade-off in Fair Representation Learning","publisher":"The Thirty-Ninth Annual Conference on Neural Information Processing Systems (NeurIPS 2025)","url":"https://arxiv.org/pdf/2508.15084","releaseDate":"TBD","summary":"It provides a quantification for the trade-offs between three categories of fairness notions, including independence, separation, and calibration, delivering an equalized-based fairness metric that helps achieve fairness for a class of downstream tasks."},{"name":"A Uniform Concentration Inequality for Kernel-Based Two-Sample Statistics","publisher":"Reject and Resubmit with Journal of Machine Learning Research","releaseDate":"2025-07","url":"https://arxiv.org/abs/2405.14051","summary":"It provides a general inequality to establish finite sample bounds for optimization problems where kernel-based statistics are involved as part of the objective function, revealing that the convergence rate or error bound for kernel-based statistics is relevant with the input dimension only up to a logarithm factor."},{"name":"Sequential and Simultaneous Distance-based Dimension Reduction","publisher":"Journal of Nonparametric Statistics","releaseDate":"2025-01-27","url":"https://www.tandfonline.com/doi/full/10.1080/10485252.2025.2451036","summary":"This work studies a stepwise dimension reduction method, built without the elliptical distribution assumption for widely studied Sufficient Dimension Reduction (Method), detecting nonlinear subspaces for both response and predictors."}],"references":[{"name":"Peng Qi","icon":"fa-solid fa-laptop"}]}